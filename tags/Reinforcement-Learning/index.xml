<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Reinforcement Learning on</title><link>https://niuez.github.io/tags/Reinforcement-Learning/</link><description>Recent content in Reinforcement Learning on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Tue, 10 Jan 2023 15:00:00 +0900</lastBuildDate><atom:link href="https://niuez.github.io/tags/Reinforcement-Learning/index.xml" rel="self" type="application/rss+xml"/><item><title>ジャックのレンタカー会社問題(強化学習第2版 4.3 方策反復)</title><link>https://niuez.github.io/notes/reinforcement_learning_ch4_3/</link><pubDate>Tue, 10 Jan 2023 15:00:00 +0900</pubDate><guid>https://niuez.github.io/notes/reinforcement_learning_ch4_3/</guid><description>ジャックのレンタカー会社問題 ジャックのレンタカー会社問題について方策反復を用いて最適方策を求めた。本と同じ結果を得ることができた。
実装は、 reinforcement_learning/main.cpp at main - niuez/reinforcement_learningに載せてある。
練習問題4.7 上の拡張を行えば良い。$V(s)$についてプロットすると以下のようになった。
各状態における行動は以下のようになった。右向きに1つ目の支店の台数、下向きに2つ目の支店の台数、行動は「1つ目の支店の台数の変化」で表している。
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 0 0 0 0 0 0 0 0 1 1 2 2 2 3 4 5 4 4 4 4 4 0 0 0 0 0 0 0 0 0 1 1 1 2 3 4 5 3 3 3 3 4 0 0 0 0 0 0 0 0 0 0 0 1 2 3 4 2 2 2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 1 2 3 1 1 1 1 2 2 2 0 0 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 -2 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 -3 -3 -2 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 -4 -3 -3 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 -4 -4 -3 -3 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 -5 -4 -4 -3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -5 -5 -4 -4 -3 -3 -2 -1 -1 -1 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 0 -5 -5 -5 -4 -4 -3 -2 -2 -2 0 0 -2 -2 -2 -2 -2 -2 -2 -2 0 0 -5 -5 -5 -5 -4 -3 -3 -3 0 0 0 1 0 0 0 0 0 0 0 0 0 -5 -5 -5 -5 -4 -4 -4 0 0 0 0 1 0 0 0 0 0 0 0 0 0 -5 -5 -5 -5 -5 -5 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 -5 -5 -5 -5 -5 -4 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 -5 -5 -5 -5 -5 -4 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 -5 -5 -5 -5 -5 -4 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 -5 -5 -5 -5 -5 -4 -3 0 0 0 0 1 0 0 0 0 0 0 0 0 0 -5 -5 -5 -5 -5 -4 -3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 高速化について この実装だと、$\sum_{s&amp;rsquo;, r} p(s&amp;rsquo;, r | s, a) (r + \gamma V(s&amp;rsquo;))$の計算に、車の最大数を$N$として$O(N^4)$かかる。</description></item><item><title>k本腕バンディット問題(強化学習第2版 2章)</title><link>https://niuez.github.io/notes/reinforcement_learning_ch2/</link><pubDate>Mon, 09 Jan 2023 15:00:00 +0900</pubDate><guid>https://niuez.github.io/notes/reinforcement_learning_ch2/</guid><description>強化学習第2版を買った 今まで焼きなまし法や遺伝的プログラミングによる、「最終的な結果に対する評価」によって最適化を行う手法を勉強してきたが、「一手一手に対する評価」はどのようにするのか興味が湧いたので勉強してみる。
k本腕バンディット問題に対するアプローチ ε-greedy法、楽観的初期値をもつε-greedy法、上限信頼区間(UCB)行動選択法、確率的勾配上昇法を実装して、得られた報酬の平均と最適行動の割合を各ステップについて計算しグラフにした。
実装は、 reinforcement_learning/main.cpp at main - niuez/reinforcement_learningに載せてある。</description></item></channel></rss>