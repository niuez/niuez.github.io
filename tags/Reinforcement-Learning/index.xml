<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Reinforcement Learning on</title><link>https://niuez.github.io/tags/Reinforcement-Learning/</link><description>Recent content in Reinforcement Learning on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Mon, 09 Jan 2023 15:00:00 +0900</lastBuildDate><atom:link href="https://niuez.github.io/tags/Reinforcement-Learning/index.xml" rel="self" type="application/rss+xml"/><item><title>k本腕バンディット問題(強化学習第2版 2章)</title><link>https://niuez.github.io/notes/reinforcement_learning_ch2/</link><pubDate>Mon, 09 Jan 2023 15:00:00 +0900</pubDate><guid>https://niuez.github.io/notes/reinforcement_learning_ch2/</guid><description>強化学習第2版を買った 今まで焼きなまし法や遺伝的プログラミングによる、「最終的な結果に対する評価」によって最適化を行う手法を勉強してきたが、「一手一手に対する評価」はどのようにするのか興味が湧いたので勉強してみる。
k本腕バンディット問題に対するアプローチ ε-greedy法、楽観的初期値をもつε-greedy法、上限信頼区間(UCB)行動選択法、確率的勾配上昇法を実装して、得られた報酬の平均と最適行動の割合を各ステップについて計算しグラフにした。
実装は、 reinforcement_learning/main.cpp at main - niuez/reinforcement_learningに載せてある。</description></item></channel></rss>