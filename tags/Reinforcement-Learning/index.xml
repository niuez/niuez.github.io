<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Reinforcement Learning on</title><link>https://niuez.github.io/tags/Reinforcement-Learning/</link><description>Recent content in Reinforcement Learning on</description><generator>Hugo -- gohugo.io</generator><language>en-us</language><lastBuildDate>Sun, 22 Jan 2023 15:00:00 +0900</lastBuildDate><atom:link href="https://niuez.github.io/tags/Reinforcement-Learning/index.xml" rel="self" type="application/rss+xml"/><item><title>深層強化学習とREINFORCE algorithmによるCartPoleのアプローチ(強化学習第2版 13 REINFORCE algorithm)</title><link>https://niuez.github.io/notes/reinforcement_learning_ch13_reinforce/</link><pubDate>Sun, 22 Jan 2023 15:00:00 +0900</pubDate><guid>https://niuez.github.io/notes/reinforcement_learning_ch13_reinforce/</guid><description>深層強化学習へ これまでの強化学習の勉強では、状態空間が配列に収まるようなものを見てきた。しかし、実数を取るものや、もっと状態空間が広いものに対応するため、深層強化学習に手をつけることにした。
つくりながら学ぶ！深層強化学習 PyTorchによる実践プログラミング - Amazonを買って勉強することにした。半分くらいは普通の強化学習を扱うので、強化学習を一から始める人にはおすすめ。自分はその半分を強化学習第2版で埋めてたので、買わなくてよかったかも&amp;hellip;
CartPole OpenAIのgymにある CartPoleという例題を用いた。カートを左右に動かして、ポールを倒さないようにするのが目的である。
PyTorchを用いて実装したものの&amp;hellip; 上の本を参考にDQNで解くことはできた。しかし、PyTorchや深層学習への経験が浅く、何がしたくてそのコードを書いているかわからず、それが原因でREINFORCE algorithmに改造する方法もわからなかったので、PyTorchのREINFORCE algorithmを参考にしつつ、一回全部C++で書くことにした(？)。
PyTorchによるREINFORCE algorithm reinforcement_learning/REINFORCE_cartpole.ipynb at main · niuez/reinforcement_learning</description></item><item><title>風が吹くグリッドワールド(強化学習第2版 6 Sarsa/Q-learning/expected Sarsa)</title><link>https://niuez.github.io/notes/reinforcement_learning_ch6/</link><pubDate>Sun, 15 Jan 2023 15:00:00 +0900</pubDate><guid>https://niuez.github.io/notes/reinforcement_learning_ch6/</guid><description>風が吹くグリッドワールド $10 \times 7$のグリッド上で$x = 0, 1, 2, 3, 4, 5, 6, 7, 8, 9$を満たすグリッドでは、それぞれ$0, 0, 0, 1, 1, 1, 2, 2, 1, 0$の強さの風が$y$のマイナスの向きに吹いている。風が吹いているマスでは、そこから移動しようとした時に風の分が加わった異動になってしまう。この世界で、$(0, 3)$から$(7, 3)$に到達する必要があるときの最善手を求めたい。移動は、4方向に限ることにした。</description></item><item><title>ギャンブラー問題(強化学習第2版 4.4 価値反復)</title><link>https://niuez.github.io/notes/reinforcement_learning_ch4_4/</link><pubDate>Wed, 11 Jan 2023 15:00:00 +0900</pubDate><guid>https://niuez.github.io/notes/reinforcement_learning_ch4_4/</guid><description>ギャンブラー問題 $p_h=0.4,0.25,0.55$それぞれのギャンブラー問題について、価値反復を用いて最適方策を求めた。
実装は、 reinforcement_learning/main.cpp at main - niuez/reinforcement_learningに載せてある。
価値関数と、最適方策を図にした。
$p_h=0.4$ $p_h=0.25$ $p_h \leq 0.5$では、どこかで賭けをして勝ちを狙いに行く必要があるっぽい。
$p_h=0.55$ $p_h=0.55$については、少しずつ掛けて勝つことができるのでこのような結果になった。</description></item><item><title>ブラックジャック問題(強化学習第2版 5 方策オン/オフ型)</title><link>https://niuez.github.io/notes/reinforcement_learning_ch5/</link><pubDate>Wed, 11 Jan 2023 15:00:00 +0900</pubDate><guid>https://niuez.github.io/notes/reinforcement_learning_ch5/</guid><description>ブラックジャック問題 方策オン型初回訪問MCと、方策オフ型重み付き重点サンプリングMCを実装した。コードは reinforcement_learning/main.cpp at main · niuez/reinforcement_learningに載せてある。両方、ソフト方策はεソフトを用いている。収束は方策オフ型の方がはやかった。
両者の違いは後者が、グリーディーな方策と、εによって選ばれたランダムな方策による価値の更新の重みが異なることだ。前者は同一に扱っている。
1: ヒット(もう一枚) 0: スティック(やめ) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 ace = 0 A23456789T 11 : 1111111111 12 : 1110001111 13 : 1000001111 14 : 1000001111 15 : 1000001111 16 : 1000001111 17 : 0000000000 18 : 0000000000 19 : 0000000000 20 : 0000000000 21 : 0000000000 ace = 1 A23456789T 11 : 0000000000 12 : 1111111111 13 : 1111111111 14 : 1111111111 15 : 1111111111 16 : 1111111111 17 : 1111111111 18 : 1000000011 19 : 0000000000 20 : 0000000000 21 : 0000000000 価値関数について、ヒットするかスティックするかの差分を計算してみた。赤ほどヒットした方がよく、青ほどスティックした方がいい。</description></item><item><title>ジャックのレンタカー会社問題(強化学習第2版 4.3 方策反復)</title><link>https://niuez.github.io/notes/reinforcement_learning_ch4_3/</link><pubDate>Tue, 10 Jan 2023 15:00:00 +0900</pubDate><guid>https://niuez.github.io/notes/reinforcement_learning_ch4_3/</guid><description>ジャックのレンタカー会社問題 ジャックのレンタカー会社問題について方策反復を用いて最適方策を求めた。本と同じ結果を得ることができた。
実装は、 reinforcement_learning/main.cpp at main - niuez/reinforcement_learningに載せてある。
練習問題4.7 上の拡張を行えば良い。$V(s)$についてプロットすると以下のようになった。
各状態における行動は以下のようになった。右向きに1つ目の支店の台数、下向きに2つ目の支店の台数、行動は「1つ目の支店の台数の変化」で表している。
1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 0 0 0 0 0 0 0 0 1 1 2 2 2 3 4 5 4 4 4 4 4 0 0 0 0 0 0 0 0 0 1 1 1 2 3 4 5 3 3 3 3 4 0 0 0 0 0 0 0 0 0 0 0 1 2 3 4 2 2 2 2 3 3 0 0 0 0 0 0 0 0 0 0 0 1 2 3 1 1 1 1 2 2 2 0 0 0 0 0 0 0 0 0 0 0 1 2 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 -2 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 -3 -3 -2 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 -4 -3 -3 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 -4 -4 -3 -3 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 -5 -4 -4 -3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 -5 -5 -4 -4 -3 -3 -2 -1 -1 -1 0 -1 -1 -1 -1 -1 -1 -1 -1 -1 0 -5 -5 -5 -4 -4 -3 -2 -2 -2 0 0 -2 -2 -2 -2 -2 -2 -2 -2 0 0 -5 -5 -5 -5 -4 -3 -3 -3 0 0 0 1 0 0 0 0 0 0 0 0 0 -5 -5 -5 -5 -4 -4 -4 0 0 0 0 1 0 0 0 0 0 0 0 0 0 -5 -5 -5 -5 -5 -5 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 -5 -5 -5 -5 -5 -4 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 -5 -5 -5 -5 -5 -4 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 -5 -5 -5 -5 -5 -4 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 -5 -5 -5 -5 -5 -4 -3 0 0 0 0 1 0 0 0 0 0 0 0 0 0 -5 -5 -5 -5 -5 -4 -3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 高速化について この実装だと、$\sum_{s&amp;rsquo;, r} p(s&amp;rsquo;, r | s, a) (r + \gamma V(s&amp;rsquo;))$の計算に、車の最大数を$N$として$O(N^4)$かかる。</description></item><item><title>k本腕バンディット問題(強化学習第2版 2章)</title><link>https://niuez.github.io/notes/reinforcement_learning_ch2/</link><pubDate>Mon, 09 Jan 2023 15:00:00 +0900</pubDate><guid>https://niuez.github.io/notes/reinforcement_learning_ch2/</guid><description>強化学習第2版を買った 今まで焼きなまし法や遺伝的プログラミングによる、「最終的な結果に対する評価」によって最適化を行う手法を勉強してきたが、「一手一手に対する評価」はどのようにするのか興味が湧いたので勉強してみる。
k本腕バンディット問題に対するアプローチ ε-greedy法、楽観的初期値をもつε-greedy法、上限信頼区間(UCB)行動選択法、確率的勾配上昇法を実装して、得られた報酬の平均と最適行動の割合を各ステップについて計算しグラフにした。
実装は、 reinforcement_learning/main.cpp at main - niuez/reinforcement_learningに載せてある。</description></item></channel></rss>